Nice â€” this source.txt is actually super reviewable. Itâ€™s an auto-generated â€œcore filesâ€ bundle served by your app (/api/source) and it includes the parts that matter: schema, storage, monitors, and UI flow.  ï¿¼

Below is a straight-up evaluation of the idea and the code, with specific fixes.

Idea: what youâ€™re building (and whatâ€™s strong)

Youâ€™re essentially building â€œresponse advertisingâ€: monitor high-intent questions in communities (Reddit, Google Alerts RSS, LinkedIn scanning), score intent with Gemini, draft a subtle helpful reply, then send it to the operator via Telegram for approval.  ï¿¼

Thatâ€™s a solid wedge because:
	â€¢	It targets people already asking (much higher conversion than cold ads).
	â€¢	The human-in-the-loop approval via Telegram is a pragmatic safety & quality layer.  ï¿¼
	â€¢	Youâ€™ve already built the core loop: monitor â†’ score â†’ draft â†’ store â†’ notify â†’ feedback.  ï¿¼

Big product risk (worth addressing in positioning + UX):
	â€¢	Many communities view â€œsubtle recommendationsâ€ as stealth marketing. Your app should ship â€œcommunity-safeâ€ modes: disclosure templates, tone guardrails, and per-platform rules.

Code review highlights

1) Data model is clean, but you need stronger typing + constraints

Your tables (businesses, campaigns, leads, ai_responses, response_feedback, seen_items) are the right shape for this product.  ï¿¼

But there are issues:
	â€¢	Type exports look broken in shared/schema.ts: e.g. export type InsertBusiness = z.infer; (missing typeof insertBusinessSchema). If thatâ€™s not just a generator formatting bug, the code wouldnâ€™t compile.  ï¿¼
	â€¢	campaigns.businessId, leads.campaignId, aiResponses.leadId are not declared with DB-level references(...) in the table definitions (you reference them only in relations). Consider enforcing foreign keys at the database schema level too.  ï¿¼
	â€¢	Add indexes that match your queries:
	â€¢	businesses.user_id
	â€¢	campaigns.business_id
	â€¢	leads.campaign_id, leads.created_at
	â€¢	ai_responses.lead_id, ai_responses.status
	â€¢	seen_items.dedup_key (you already have unique ğŸ‘)  ï¿¼

2) Storage layer has N+1 query patterns (will bite you)

Your DatabaseStorage.getCampaignsByUser, getLeadsByCampaigns, and getResponsesByLeads loop and query per ID. Thatâ€™s N+1 and will slow down fast with real volume.  ï¿¼

Fix: use inArray() (or joins) and fetch in one query per table.

3) Security footguns: CORS * + public â€œlead ingestionâ€ endpoints

You set permissive CORS headers for /api/li-scan (Access-Control-Allow-Origin: "*") and accept POSTs. Thatâ€™s an easy target for abuse (any site can hammer your endpoint).  ï¿¼

At minimum:
	â€¢	restrict origin to your domain(s),
	â€¢	require an HMAC token / signed payload,
	â€¢	rate limit by IP and/or token.

4) HTML escaping looks wrong (possible injection / broken Telegram formatting)

Your escapeHtml / escHtml functions appear malformed:

return text.replace(/&/g, "&").replace(//g, ">");

That missing pattern means < / > arenâ€™t actually being escaped (and & replacement is also suspiciously incomplete). If this is real in source (not a bundle formatting artifact), itâ€™s a bug + potential injection vector in Telegram messages.  ï¿¼

You want something like: replace &, <, >, ", ' (or use Telegramâ€™s MarkdownV2 escaping correctly).

5) Prompting is on the right track, but you should harden it

Whatâ€™s good:
	â€¢	You do explicit JSON output for intent scoring.  ï¿¼
	â€¢	You use feedback history to adjust â€œsalesy vs subtleâ€ behavior. Thatâ€™s a great loop.  ï¿¼
	â€¢	You cap token output for scoring (maxOutputTokens: 512) and keep the â€œpost contentâ€ excerpt bounded.  ï¿¼

What to improve:
	â€¢	Your JSON parse is regex-based (match(/\{[\s\S]*\}/)) which is fragile. Prefer a strict â€œJSON onlyâ€ response plus a safe parser, or use a tool/function calling schema if available.
	â€¢	Add a â€œno hallucinated factsâ€ rule: the assistant must not invent credentials, addresses, prices, results, etc. (This matters a lot for stealthy recommendations.)

6) â€œSource downloadâ€ endpoints are a cool dev feature â€” but lock them down

You have /api/source dumping core files and /api/download/source serving a base64 tar.gz archive. Thatâ€™s awesome for AI review, but in production it should be:
	â€¢	disabled,
	â€¢	or behind admin auth,
	â€¢	or gated by an env flag.  ï¿¼

Otherwise youâ€™re inviting automated scraping of your internals.

UX/product flow review (front-end)

The Landing â†’ Onboarding â†’ Dashboard story is coherent:
	â€¢	Landing clearly states the value proposition and â€œapprove via Telegramâ€ loop.  ï¿¼
	â€¢	Onboarding collects business info, then calls /api/strategy/generate.  ï¿¼
	â€¢	Dashboard pulls businesses/campaigns/leads and shows status.  ï¿¼

One suggestion: surface why a lead was matched (the intent reasoning) right next to the draft response, so the user can trust the systemâ€™s judgment (you already generate â€œreasoningâ€ for Reddit matches).  ï¿¼

What Iâ€™d prioritize next (highest ROI)
	1.	Fix escaping + Telegram formatting (this is correctness + safety).  ï¿¼
	2.	Lock down /api/li-scan, /api/source, /api/download/source (auth + origin + rate limits).  ï¿¼
	3.	Eliminate N+1 queries in storage; your dashboard will feel instantly faster at scale.  ï¿¼
	4.	DB constraints + indexes to match your access patterns.  ï¿¼
	5.	Prompt hardening: strict JSON parsing + anti-hallucination + community-safe templates.  ï¿¼

If you want, paste (or link raw) server/routes.ts and server/telegram.ts next â€” those two will let me do a deeper review of auth boundaries, request validation, and platform-abuse protection.